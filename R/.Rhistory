geom_point(aes(size=n, colour=model), alpha=0.6) +
coord_flip(clip = "off") +
theme(legend.position="top", legend.box="vertical",legend.margin = margin(),
plot.margin = ggplot2::margin(0, 50, 2, 2, "pt"),
axis.ticks = element_blank()) +
ylab("recall (# CPMI arc = gold arc)/(# gold arcs)") +
xlab("gold dependency label (ordered by mean arc length)") +
ggtitle("arc length > 1") +
theme(plot.title = element_text(hjust = 0.5),
legend.spacing = unit(0, 'cm'))
p.rel.gt1 +ggtitle("Accuracy (recall) by gold label (n>50, arc length > 1)")
xlnet.len.gold <- prepare_by_len_gold(read_csv("by_wordpair/wordpair_abs-loaded=xlnet-base-cased_pad30_2020-07-05-17-41.csv"))
bert.len.gold <-  prepare_by_len_gold(read_csv("by_wordpair/wordpair_abs-loaded=bert-large-cased_pad60_2020-07-05-16-29.csv"))
xlm.len.gold <-   prepare_by_len_gold(read_csv("by_wordpair/wordpair_abs-loaded=xlm-mlm-en-2048_pad60_2020-07-05-17-29.csv"))
bart.len.gold <-  prepare_by_len_gold(read_csv("by_wordpair/wordpair_abs-loaded=bart-large_pad60_2020-07-05-16-06.csv"))
# gpt2.len.gold <-  prepare_by_len_gold(read_csv("by_wordpair/wordpair_gpt2_pad30_2020-04-24-13-45.csv"))
dbert.len.gold <- prepare_by_len_gold(read_csv("by_wordpair/wordpair_abs-loaded=distilbert-base-cased_pad60_2020-07-05-17-05.csv"))
w2v.len.gold <-   prepare_by_len_gold(read_csv("by_wordpair/wordpair_abs-loaded=w2v_pad0_2020-07-05-17-17.csv"))
# All five models in one df
five.len.gold <- join_five(dbert.len.gold,bart.len.gold,bert.len.gold,xlnet.len.gold,w2v.len.gold,
by = c("n","lin_dist"),
suffixes=c(".DistilBERT",".Bart",".BERT",".XLNet",".Word2Vec"))
# A plot exploring accuracy by lin_dist
p.lin_dist <-
five.len.gold %>% filter(n>25) %>%
ggplot(aes(y=pct_acc, x=lin_dist)) +
geom_text(aes(label=n, y=Inf), hjust=0, size=2.5, colour="grey") +
annotate("text",x=Inf,y=Inf, label="n", size=2.5, hjust=0, vjust=0, colour="grey") +
geom_line(aes(group=lin_dist), colour="grey") +
geom_point(aes(size=n, colour=model), alpha=0.7) +
coord_flip(clip = "off") +
theme(legend.position="top", legend.box="vertical",legend.margin = margin(),
plot.margin = ggplot2::margin(0, 50, 0, 2, "pt"),
legend.spacing = unit(0, 'cm')
) + scale_x_continuous(trans="identity",breaks = seq(1, 23, by = 2), minor_breaks = seq(1, 23, by = 1)) +
ylab("recall (# CPMI arc = gold arc)/(# gold arcs)") +
xlab("arc length") +
ggtitle("Accuracy (recall) by arc length (n>25)")
# PRED PMI LEN
prepare_by_len_pred <- function(dataframe){
#' Prepare csv as df data grouped by 'lin_dist'
len = dataframe %>% filter(pmi_edge_sum==T) %>%
group_by(lin_dist) %>% summarise(meanpmi=mean(pmi_sum), varpmi=var(pmi_sum), n=n())
dataframe = dataframe %>% filter(pmi_edge_sum==T) %>%
mutate(acc=gold_edge==pmi_edge_sum) %>%
group_by(lin_dist,acc) %>% summarise(n=n()) %>%
pivot_wider(names_from = acc, names_prefix = "pmi", values_from = c(n), values_fill = list(n = 0)) %>%
left_join(len, by="lin_dist") %>%
mutate(pct_acc = pmiTRUE/n)
return(dataframe)
}
xlnet.len.pred <- prepare_by_len_pred(read_csv("by_wordpair/wordpair_abs-loaded=xlnet-base-cased_pad30_2020-07-05-17-41.csv"))
bert.len.pred <-  prepare_by_len_pred(read_csv("by_wordpair/wordpair_abs-loaded=bert-large-cased_pad60_2020-07-05-16-29.csv"))
xlm.len.pred <-   prepare_by_len_pred(read_csv("by_wordpair/wordpair_abs-loaded=xlm-mlm-en-2048_pad60_2020-07-05-17-29.csv"))
bart.len.pred <-  prepare_by_len_pred(read_csv("by_wordpair/wordpair_abs-loaded=bart-large_pad60_2020-07-05-16-06.csv"))
# gpt2.len.pred <-  prepare_by_len_pred(read_csv("by_wordpair/wordpair_gpt2_pad30_2020-04-24-13-45.csv"))
dbert.len.pred <- prepare_by_len_pred(read_csv("by_wordpair/wordpair_abs-loaded=distilbert-base-cased_pad60_2020-07-05-17-05.csv"))
w2v.len.pred <-   prepare_by_len_pred(read_csv("by_wordpair/wordpair_abs-loaded=w2v_pad0_2020-07-05-17-17.csv"))
# All five models in one df
five.len.pred <- join_five(dbert.len.pred,bart.len.pred,bert.len.pred,xlnet.len.pred,w2v.len.pred,
by = c("n","lin_dist"),
suffixes=c(".DistilBERT",".Bart",".BERT",".XLNet",".Word2Vec"))
# A plot exploring accuracy by lin_dist
p.lin_dist.pred <-
five.len.pred %>%  filter(n>25) %>%
ggplot(aes(y=pct_acc, x=lin_dist)) +
#geom_text(aes(label=n, y=Inf), hjust=0, size=3, colour="grey") +
#annotate("text",x=Inf,y=Inf, label="n", size=3, hjust=0, vjust=0, colour="grey") +
#geom_line(aes(group=lin_dist), colour="grey") +
geom_point(aes(size=n, colour=model), alpha=0.8) +
coord_flip(clip = "off") +
theme(legend.position="top", legend.box="vertical", legend.margin = margin(),
plot.margin = ggplot2::margin(0, 20, 2, 0, "pt"),
legend.spacing = unit(0, 'cm')
) + scale_x_continuous(trans="identity", breaks = seq(1, 35, by = 4), minor_breaks = seq(1, 35, by = 1)) +
ylab("precision (# CPMI arc = gold arc)/(# CPMI arcs)") +
xlab("arc length") +
ggtitle("Accuracy (precision) by arc length (n>25)")
grid.arrange(p.lin_dist,p.lin_dist.pred,ncol=2,widths=c(10,9))
grid.arrange(p.pmi.gold,p.pmi.pred,ncol=2)
c(
get_lgppl("../results-clean/bert-base-uncased.ckpt-10k_pad30_2020-07-06-00-45/scores_bert-base-uncased.ckpt-10k_pad30_2020-07-06-00-45.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-50k_pad30_2020-07-06-01-48/scores_bert-base-uncased.ckpt-50k_pad30_2020-07-06-01-48.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-100k_pad30_2020-07-06-02-51/scores_bert-base-uncased.ckpt-100k_pad30_2020-07-06-02-51.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-500k_pad30_2020-07-06-03-53/scores_bert-base-uncased.ckpt-500k_pad30_2020-07-06-03-53.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-1000k_pad30_2020-07-06-04-56/scores_bert-base-uncased.ckpt-1000k_pad30_2020-07-06-04-56.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-1500k_pad30_2020-07-06-05-58/scores_bert-base-uncased.ckpt-1500k_pad30_2020-07-06-05-58.csv"),
get_lgppl("../results-clean/bert-base-uncased_pad30_2020-07-04-12-15/scores_bert-base-uncased_pad30_2020-07-04-12-15.csv"))
get_lgppl<-function(csvfile){
mean_ppl<-read_csv(csvfile) %>%
mutate(sentence_logperplexity=-pseudo_loglik/sentence_length) %>%
select(sentence_index,sentence_logperplexity) %>%
summarize(mean_ppl=mean(sentence_logperplexity))
return(mean_ppl[[1]])
}
c(
get_lgppl("../results-clean/bert-base-uncased.ckpt-10k_pad30_2020-07-06-00-45/scores_bert-base-uncased.ckpt-10k_pad30_2020-07-06-00-45.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-50k_pad30_2020-07-06-01-48/scores_bert-base-uncased.ckpt-50k_pad30_2020-07-06-01-48.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-100k_pad30_2020-07-06-02-51/scores_bert-base-uncased.ckpt-100k_pad30_2020-07-06-02-51.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-500k_pad30_2020-07-06-03-53/scores_bert-base-uncased.ckpt-500k_pad30_2020-07-06-03-53.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-1000k_pad30_2020-07-06-04-56/scores_bert-base-uncased.ckpt-1000k_pad30_2020-07-06-04-56.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-1500k_pad30_2020-07-06-05-58/scores_bert-base-uncased.ckpt-1500k_pad30_2020-07-06-05-58.csv"),
get_lgppl("../results-clean/bert-base-uncased_pad30_2020-07-04-12-15/scores_bert-base-uncased_pad30_2020-07-04-12-15.csv"))
library(tidyverse)
get_lgppl<-function(csvfile){
mean_ppl<-read_csv(csvfile) %>%
mutate(sentence_logperplexity=-pseudo_loglik/sentence_length) %>%
select(sentence_index,sentence_logperplexity) %>%
summarize(mean_ppl=mean(sentence_logperplexity))
return(mean_ppl[[1]])
}
c(
get_lgppl("../results-clean/bert-base-uncased.ckpt-10k_pad30_2020-07-06-00-45/scores_bert-base-uncased.ckpt-10k_pad30_2020-07-06-00-45.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-50k_pad30_2020-07-06-01-48/scores_bert-base-uncased.ckpt-50k_pad30_2020-07-06-01-48.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-100k_pad30_2020-07-06-02-51/scores_bert-base-uncased.ckpt-100k_pad30_2020-07-06-02-51.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-500k_pad30_2020-07-06-03-53/scores_bert-base-uncased.ckpt-500k_pad30_2020-07-06-03-53.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-1000k_pad30_2020-07-06-04-56/scores_bert-base-uncased.ckpt-1000k_pad30_2020-07-06-04-56.csv"),
get_lgppl("../results-clean/bert-base-uncased.ckpt-1500k_pad30_2020-07-06-05-58/scores_bert-base-uncased.ckpt-1500k_pad30_2020-07-06-05-58.csv"),
get_lgppl("../results-clean/bert-base-uncased_pad30_2020-07-04-12-15/scores_bert-base-uncased_pad30_2020-07-04-12-15.csv"))
# bertckpt <- read_csv("bert-checkpoint500.csv")
bertckpt <- read_csv("abs-bert-checkpoint.csv")
# bertckpt <- read_csv("bert-checkpoint500.csv")
bertckpt <- read_csv("abs-bert-checkpoint.csv")
bertckpt <- filter(bertckpt,`training steps`!='off-shelf')
bertckpt$`training steps` <- fct_relevel(bertckpt$`training steps`, c("10","50","100","500","1000","1500"))
ggcolhue <- function(n) {
hues = seq(15, 375, length = n + 1)
hcl(h = hues, l = 65, c = 100)[1:n]
}
ggcolhue(2)[[1]]
coeff <- 20
bertckpt %>%
rename(nproj = nproj_sum, proj = proj_sum) %>%
pivot_longer(cols = -c(`training steps`,meanppl),
names_to = "method",
values_to = "accuracy") %>%
filter(method %in% c("nproj","proj")) %>%
ggplot(aes(x=`training steps`,y=accuracy * coeff,colour=method)) +
geom_path(aes(y = meanppl,group=rep(c("log PPL"))), color="#888888") +
geom_point() + geom_line(aes(group=method)) +
geom_hline(yintercept=.26 * coeff, linetype="dashed", color = ggcolhue(2)[[2]]) +
geom_hline(yintercept=.13 * coeff, linetype="dashed", color = ggcolhue(2)[[1]]) +
#geom_hline(yintercept=.5  * coeff, linetype="dashed") +
xlab("BERT training steps (thousands)") +
scale_y_continuous(
name = "mean log perplexity",
sec.axis = sec_axis(~./coeff, name="accuracy")
) +
theme( axis.ticks.y.left = element_line(color = "#666666"),
axis.text.y.left = element_text(color =  "#666666"),
axis.title.y.left = element_text(color = "#666666")) +
scale_color_discrete(name="CPMI\ndependency\nmethod") +
ggtitle("Accuracy and perplexity during training")
bertckpt %>%
rename(nproj = nproj_sum, proj = proj_sum) %>%
ggplot() +
geom_point(aes(x=`training steps`,y=meanppl)) +
xlab("training steps (bert-base-uncased)") +
ylab("Avg sentence log perplexity") +
ggtitle("Perplexity at checkpoints during training")
get_lgppl<-function(csvfile){
mean_ppl<-read_csv(csvfile) %>%
mutate(sentence_logperplexity=-pseudo_loglik/sentence_length) %>%
select(sentence_index,sentence_logperplexity) %>%
summarize(mean_ppl=mean(sentence_logperplexity))
return(mean_ppl[[1]])
}
bertckpt %>%
rename(nproj = nproj_sum, proj = proj_sum) %>%
pivot_longer(cols = -c(`training steps`,meanppl),
names_to = "method",
values_to = "accuracy") %>%
filter(method %in% c("nproj","proj")) %>%
ggplot(aes(x=`training steps`,y=accuracy * coeff,colour=method)) +
geom_path(aes(y = meanppl,group=rep(c("log PPL"))), color="#888888") +
geom_point() + geom_line(aes(group=method)) +
geom_hline(yintercept=.26 * coeff, linetype="dashed", color = ggcolhue(2)[[2]]) +
geom_hline(yintercept=.13 * coeff, linetype="dashed", color = ggcolhue(2)[[1]]) +
#geom_hline(yintercept=.5  * coeff, linetype="dashed") +
xlab("BERT training steps (thousands)") +
scale_y_continuous(
name = "mean log perplexity",
sec.axis = sec_axis(~./coeff, name="accuracy")
) +
theme( axis.ticks.y.left = element_line(color = "#666666"),
axis.text.y.left = element_text(color =  "#666666"),
axis.title.y.left = element_text(color = "#666666")) +
scale_color_discrete(name="CPMI\ndependency\nmethod") +
ggtitle("Accuracy and perplexity during training")
theme_set(theme_minimal())
bertckpt %>%
rename(nproj = nproj_sum, proj = proj_sum) %>%
pivot_longer(cols = -c(`training steps`,meanppl),
names_to = "method",
values_to = "accuracy") %>%
filter(method %in% c("nproj","proj")) %>%
ggplot(aes(x=`training steps`,y=accuracy * coeff,colour=method)) +
geom_path(aes(y = meanppl,group=rep(c("log PPL"))), color="#888888") +
geom_point() + geom_line(aes(group=method)) +
geom_hline(yintercept=.26 * coeff, linetype="dashed", color = ggcolhue(2)[[2]]) +
geom_hline(yintercept=.13 * coeff, linetype="dashed", color = ggcolhue(2)[[1]]) +
#geom_hline(yintercept=.5  * coeff, linetype="dashed") +
xlab("BERT training steps (thousands)") +
scale_y_continuous(
name = "mean log perplexity",
sec.axis = sec_axis(~./coeff, name="accuracy")
) +
theme( axis.ticks.y.left = element_line(color = "#666666"),
axis.text.y.left = element_text(color =  "#666666"),
axis.title.y.left = element_text(color = "#666666")) +
scale_color_discrete(name="CPMI\ndependency\nmethod") +
ggtitle("Accuracy and perplexity during training")
bertckpt <- read_csv("bert-checkpoint500.csv")
bertckpt <- filter(bertckpt,`training steps`!='off-shelf')
bertckpt$`training steps` <- fct_relevel(bertckpt$`training steps`, c("10","50","100","500","1000","1500"))
ggcolhue <- function(n) {
hues = seq(15, 375, length = n + 1)
hcl(h = hues, l = 65, c = 100)[1:n]
}
ggcolhue(2)[[1]]
coeff <- 20
bertckpt %>%
rename(nproj = nproj_sum, proj = proj_sum) %>%
pivot_longer(cols = -c(`training steps`,meanppl),
names_to = "method",
values_to = "accuracy") %>%
filter(method %in% c("nproj","proj")) %>%
ggplot(aes(x=`training steps`,y=accuracy * coeff,colour=method)) +
geom_path(aes(y = meanppl,group=rep(c("log PPL"))), color="#888888") +
geom_point() + geom_line(aes(group=method)) +
geom_hline(yintercept=.26 * coeff, linetype="dashed", color = ggcolhue(2)[[2]]) +
geom_hline(yintercept=.13 * coeff, linetype="dashed", color = ggcolhue(2)[[1]]) +
#geom_hline(yintercept=.5  * coeff, linetype="dashed") +
xlab("BERT training steps (thousands)") +
scale_y_continuous(
name = "mean log perplexity",
sec.axis = sec_axis(~./coeff, name="accuracy")
) +
theme( axis.ticks.y.left = element_line(color = "#666666"),
axis.text.y.left = element_text(color =  "#666666"),
axis.title.y.left = element_text(color = "#666666")) +
scale_color_discrete(name="CPMI\ndependency\nmethod") +
ggtitle("Accuracy and perplexity during training")
scores_models %>% filter(model %in% selected_models) %>% filter(model!="Word2Vec") %>%
filter(score_method %in% c("projective.uuas.sum"),
sentence_length > 4,
sentence_logperplexity<4*exp(1)
) %>%
ggplot(aes(x=sentence_logperplexity,y=uuas)) + theme(legend.position = "top") +
geom_point(alpha=.15) + geom_smooth(method = "lm", formula = y~x) +
#geom_boxplot(aes(x=0),alpha=0.15) +
scale_y_continuous(breaks=c(0,.5,1)) +
facet_grid(model~.) +
ggpmisc::stat_poly_eq(aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~~")),
label.x = "right", label.y = 1,
formula = y~x, parse = TRUE, size = 3) +
labs(x="log(pseudo-perplexity)", y="CPMI dependency accuracy") +
ggtitle("Accuracy vs LM performance")
# With absolute value
scores_bert_large_60 <-  prepareppl(read.csv("by_sentence/scores_abs-loaded=bert-large-cased_pad60_2020-07-05-16-29.csv"))
scores_xlnet_base_30 <-  prepareppl(read_csv("by_sentence/scores_abs-loaded=xlnet-base-cased_pad30_2020-07-05-17-41.csv"))
scores_xlm_60 <-         prepareppl(read_csv("by_sentence/scores_abs-loaded=xlm-mlm-en-2048_pad60_2020-07-05-17-29.csv"))
scores_bart_60 <-        prepareppl(read_csv("by_sentence/scores_abs-loaded=bart-large_pad60_2020-07-05-16-06.csv"))
scores_dbert_60 <-        prepareppl(read_csv("by_sentence/scores_abs-loaded=distilbert-base-cased_pad60_2020-07-05-17-05.csv"))
prepareppl <- function(csv){
#' Prepare the raw data
df = csv %>%
mutate(sentence_logperplexity=-pseudo_loglik/sentence_length) %>%
select(sentence_index, sentence_length, number_edges, pseudo_loglik, sentence_logperplexity,
baseline_linear, baseline_random_proj, baseline_random_nonproj,
# too many types of scores makes it messy. just choosing the 'sum' types
projective.uuas.sum, projective.uuas.tril, projective.uuas.triu)  %>%
pivot_longer(
cols = -c(sentence_index, sentence_length, number_edges, pseudo_loglik, sentence_logperplexity),
names_to = "score_method", values_to = "uuas")
return(df)
}
# With absolute value
scores_bert_large_60 <-  prepareppl(read.csv("by_sentence/scores_abs-loaded=bert-large-cased_pad60_2020-07-05-16-29.csv"))
scores_xlnet_base_30 <-  prepareppl(read_csv("by_sentence/scores_abs-loaded=xlnet-base-cased_pad30_2020-07-05-17-41.csv"))
scores_xlm_60 <-         prepareppl(read_csv("by_sentence/scores_abs-loaded=xlm-mlm-en-2048_pad60_2020-07-05-17-29.csv"))
scores_bart_60 <-        prepareppl(read_csv("by_sentence/scores_abs-loaded=bart-large_pad60_2020-07-05-16-06.csv"))
scores_dbert_60 <-        prepareppl(read_csv("by_sentence/scores_abs-loaded=distilbert-base-cased_pad60_2020-07-05-17-05.csv"))
scores_w2v <-            prepareppl(read_csv("by_sentence/scores_abs-loaded=w2v_pad0_2020-07-05-17-17.csv"))
scores_bert_base_30 <-  prepareppl(read.csv("by_sentence/scores_abs-loaded=bert-base-cased_pad30_2020-07-05-16-17.csv"))
scores_xlnet_large_30 <- prepareppl(read_csv("by_sentence/scores_abs-loaded=xlnet-large-cased_pad30_2020-07-05-17-53.csv"))
scores_bert_base_60$model <- 'BERT base'
scores_bert_base_30$model <- 'BERT base'
scores_bert_large_60$model <- 'BERT large'
scores_xlnet_base_30$model <- 'XLNet base'
scores_xlnet_large_30$model <- 'XLNet large'
scores_xlm_60$model <- 'XLM'
scores_bart_60$model <- 'Bart'
scores_gpt2_30$model <- 'GPT2'
scores_dbert_60$model <- 'DistilBERT'
scores_w2v$model <- 'Word2Vec'
scores_models = rbind(scores_dbert_60,
scores_bert_base_30,
scores_bert_large_60,
scores_xlnet_base_30,
scores_xlnet_large_30,
scores_xlm_60,
scores_bart_60,
scores_gpt2_30,
scores_w2v)
scores_models$model <- factor(scores_models$model,
levels = c("DistilBERT",
"BERT base",
"BERT large",
"XLNet base",
"XLNet large",
"XLM",
"Bart",
"GPT2",
"Word2Vec"))
scores_models %>% filter(sentence_index == 0,score_method == 'projective.uuas.sum')
scores_models %>% filter(sentence_length %in% 4:61,score_method == 'projective.uuas.sum') %>%
ggplot(mapping = aes(x=sentence_length)) +
scale_x_continuous(trans='log10') +
geom_smooth(aes(y=uuas, colour=model), alpha = 1/5) +
# ylim(0,1) + #facet_grid(model~.) +
ggtitle("Accuracy vs sentence length (5-60)") +
labs(x="sentence length",
y="PMI dependency accuracy")
selected_models <- c('DistilBERT','BERT large','BERT base', 'XLNet large','XLNet base', 'XLM', 'Bart', 'Word2Vec')
library(stringr)
baselines_violin <- scores_models %>%
filter(model=="XLM", str_detect(score_method,"^baseline")) %>%
ggplot(aes(x=score_method,y=uuas)) +
geom_violin() + geom_boxplot(width=0.1) +
scale_x_discrete(labels=c('linear','random','random proj.')) +
labs(x="baseline", y="dependency accuracy (UUAS)") +
ggtitle("Baselines")
w2v_violin <- scores_models %>%
filter(score_method == 'projective.uuas.sum', model=="Word2Vec") %>%
# filter(str_detect(score_method,"^proj")) %>%
ggplot(aes(x=model,
# colour=score_method,
y=uuas)) +
geom_violin() + geom_boxplot(width=0.1) + # facet_wrap(~model,nrow = 1) +
theme(axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) + xlab("") +
ggtitle("")
models_violin <- scores_models %>%
filter(score_method == 'projective.uuas.sum', model %in% setdiff(selected_models,c("Word2Vec"))) %>%
# filter(str_detect(score_method,"^proj")) %>%
ggplot(aes(x=model,
# colour=score_method,
y=uuas)) +
geom_violin() + geom_boxplot(width=0.1) + # facet_wrap(~model,nrow = 1) +
theme(axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) + xlab("model") +
ggtitle("Contextualized PMI models")
library(gridExtra)
grid.arrange(arrangeGrob(baselines_violin,w2v_violin,models_violin,widths=c(3,1,7)))
baselines_violin <- scores_models %>%
filter(model=="XLM", str_detect(score_method,"^baseline")) %>%
ggplot(aes(x=score_method,y=uuas)) +
geom_violin() + geom_boxplot(width=0.1) +
scale_x_discrete(labels=c('linear','random','random proj.')) +
labs(x="baseline", y="dependency accuracy (UUAS)") +
ggtitle("Baselines")
scores_models %>% filter(sentence_index == 0,score_method == 'projective.uuas.sum')
scores_models = rbind(scores_dbert_60,
scores_bert_base_30,
scores_bert_large_60,
scores_xlnet_base_30,
scores_xlnet_large_30,
scores_xlm_60,
scores_bart_60,
scores_gpt2_30,
scores_w2v)
# scores_bert_base_60 <-  prepareppl(read.csv("by_sentence/scores_bert-base-cased_pad60_2020-03-31-17-30.csv"))
# scores_xlnet_large_30 <- prepareppl(read_csv("by_sentence/scores_xlnet-large-cased_pad30_2020-04-01-12-43.csv"))
scores_gpt2_30 <-        prepareppl(read_csv("by_sentence/scores_gpt2_pad30_2020-04-24-13-45.csv"))      # note triu is best.
scores_bert_base_60$model <- 'BERT base'
scores_bert_base_30$model <- 'BERT base'
scores_bert_large_60$model <- 'BERT large'
scores_xlnet_base_30$model <- 'XLNet base'
scores_xlnet_large_30$model <- 'XLNet large'
scores_xlm_60$model <- 'XLM'
scores_bart_60$model <- 'Bart'
scores_gpt2_30$model <- 'GPT2'
scores_dbert_60$model <- 'DistilBERT'
scores_w2v$model <- 'Word2Vec'
scores_models = rbind(scores_dbert_60,
scores_bert_base_30,
scores_bert_large_60,
scores_xlnet_base_30,
scores_xlnet_large_30,
scores_xlm_60,
scores_bart_60,
scores_gpt2_30,
scores_w2v)
scores_models$model <- factor(scores_models$model,
levels = c("DistilBERT",
"BERT base",
"BERT large",
"XLNet base",
"XLNet large",
"XLM",
"Bart",
"GPT2",
"Word2Vec"))
scores_models %>% filter(sentence_index == 0,score_method == 'projective.uuas.sum')
scores_models %>% filter(sentence_length %in% 4:61,score_method == 'projective.uuas.sum') %>%
ggplot(mapping = aes(x=sentence_length)) +
scale_x_continuous(trans='log10') +
geom_smooth(aes(y=uuas, colour=model), alpha = 1/5) +
# ylim(0,1) + #facet_grid(model~.) +
ggtitle("Accuracy vs sentence length (5-60)") +
labs(x="sentence length",
y="PMI dependency accuracy")
selected_models <- c('DistilBERT','BERT large','BERT base', 'XLNet large','XLNet base', 'XLM', 'Bart', 'Word2Vec')
library(stringr)
baselines_violin <- scores_models %>%
filter(model=="XLM", str_detect(score_method,"^baseline")) %>%
ggplot(aes(x=score_method,y=uuas)) +
geom_violin() + geom_boxplot(width=0.1) +
scale_x_discrete(labels=c('linear','random','random proj.')) +
labs(x="baseline", y="dependency accuracy (UUAS)") +
ggtitle("Baselines")
w2v_violin <- scores_models %>%
filter(score_method == 'projective.uuas.sum', model=="Word2Vec") %>%
# filter(str_detect(score_method,"^proj")) %>%
ggplot(aes(x=model,
# colour=score_method,
y=uuas)) +
geom_violin() + geom_boxplot(width=0.1) + # facet_wrap(~model,nrow = 1) +
theme(axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) + xlab("") +
ggtitle("")
models_violin <- scores_models %>%
filter(score_method == 'projective.uuas.sum', model %in% setdiff(selected_models,c("Word2Vec"))) %>%
# filter(str_detect(score_method,"^proj")) %>%
ggplot(aes(x=model,
# colour=score_method,
y=uuas)) +
geom_violin() + geom_boxplot(width=0.1) + # facet_wrap(~model,nrow = 1) +
theme(axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) + xlab("model") +
ggtitle("Contextualized PMI models")
library(gridExtra)
grid.arrange(arrangeGrob(baselines_violin,w2v_violin,models_violin,widths=c(3,1,7)))
## exploring the pseudo loglikelihood measure
##
scores_models <- scores_models %>%
mutate(model = relevel(model, "Bart")) %>% mutate(model = relevel(model, "GPT2"))
scores_models %>% filter(model %in% selected_models) %>% filter(model!="Word2Vec") %>%
filter(score_method %in% c("projective.uuas.sum"),
sentence_length > 4,
sentence_logperplexity<4*exp(1)
) %>%
ggplot(aes(x=sentence_logperplexity,y=uuas)) + theme(legend.position = "top") +
geom_point(alpha=.15) + geom_smooth(method = "lm", formula = y~x) +
#geom_boxplot(aes(x=0),alpha=0.15) +
scale_y_continuous(breaks=c(0,.5,1)) +
facet_grid(model~.) +
ggpmisc::stat_poly_eq(aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~~")),
label.x = "right", label.y = 1,
formula = y~x, parse = TRUE, size = 3) +
labs(x="log(pseudo-perplexity)", y="CPMI dependency accuracy") +
ggtitle("Accuracy vs LM performance")
scores_models %>% filter(model %in% selected_models) %>% filter(model!="Word2Vec") %>%
filter(score_method %in% c("projective.uuas.sum"),
sentence_length > 4,
sentence_logperplexity<4.5*exp(1)
) %>%
ggplot(aes(x=sentence_logperplexity,y=uuas)) + theme(legend.position = "top") +
geom_point(alpha=.15) + geom_smooth(method = "lm", formula = y~x) +
#geom_boxplot(aes(x=0),alpha=0.15) +
scale_y_continuous(breaks=c(0,.5,1)) +
facet_grid(model~.) +
ggpmisc::stat_poly_eq(aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~~")),
label.x = "right", label.y = 1,
formula = y~x, parse = TRUE, size = 3) +
labs(x="log(pseudo-perplexity)", y="CPMI dependency accuracy") +
ggtitle("Accuracy vs LM performance")
scores_models %>% filter(model %in% selected_models) %>% filter(model!="Word2Vec") %>%
filter(score_method %in% c("projective.uuas.sum"),
sentence_length > 4,
sentence_logperplexity<4.2*exp(1)
) %>%
ggplot(aes(x=sentence_logperplexity,y=uuas)) + theme(legend.position = "top") +
geom_point(alpha=.15) + geom_smooth(method = "lm", formula = y~x) +
#geom_boxplot(aes(x=0),alpha=0.15) +
scale_y_continuous(breaks=c(0,.5,1)) +
facet_grid(model~.) +
ggpmisc::stat_poly_eq(aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~~")),
label.x = "right", label.y = 1,
formula = y~x, parse = TRUE, size = 3) +
labs(x="log(pseudo-perplexity)", y="CPMI dependency accuracy") +
ggtitle("Accuracy vs LM performance")
scores_models %>% filter(model %in% selected_models) %>% filter(model!="Word2Vec") %>%
filter(score_method %in% c("projective.uuas.sum"),
sentence_length > 4,
sentence_logperplexity<4*exp(1)
) %>%
ggplot(aes(x=sentence_logperplexity,y=uuas)) + theme(legend.position = "top") +
geom_point(alpha=.15) + geom_smooth(method = "lm", formula = y~x) +
#geom_boxplot(aes(x=0),alpha=0.15) +
scale_y_continuous(breaks=c(0,.5,1)) +
facet_grid(model~.) +
ggpmisc::stat_poly_eq(aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~~")),
label.x = "right", label.y = 1,
formula = y~x, parse = TRUE, size = 3) +
labs(x="log(pseudo-perplexity)", y="CPMI dependency accuracy") +
ggtitle("Accuracy vs LM performance")
