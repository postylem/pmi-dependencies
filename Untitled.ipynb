{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"results/distilbert-base-cased(5)_pad10_2020-07-05-12-41/\" \n",
    "pmis = np.load(RESULTS_DIR + 'pmi_matrices.npz')\n",
    "loglik_npz = np.load(RESULTS_DIR + 'pseudo_logliks.npz')\n",
    "\n",
    "sentences = list(pmis.keys())\n",
    "matrices = list(pmis.values())\n",
    "logliks = list(loglik_npz.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.lib.npyio.NpzFile"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pmis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0 We 're about to see if advertising works .\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmis.files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-large-cased'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_info(directory, key):\n",
    "    ''' gets specified spec value from info.txt'''\n",
    "    with open(directory+'info.txt','r') as f:\n",
    "        for l in f:\n",
    "            if l.split()[0]==key+':':\n",
    "                return(l.split()[1])\n",
    "\n",
    "get_info(RESULTS_DIR, 'model_spec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class CONLLReader():\n",
    "    def __init__(self, conll_cols, additional_field_name=None):\n",
    "        if additional_field_name:\n",
    "            conll_cols += [additional_field_name]\n",
    "        self.conll_cols = conll_cols\n",
    "        self.observation_class = namedtuple(\"Observation\", conll_cols)\n",
    "        self.additional_field_name = additional_field_name\n",
    "\n",
    "    # Data input\n",
    "    @staticmethod\n",
    "    def generate_lines_for_sent(lines):\n",
    "        '''Yields batches of lines describing a sentence in conllx.\n",
    "\n",
    "        Args:\n",
    "            lines: Each line of a conllx file.\n",
    "        Yields:\n",
    "            a list of lines describing a single sentence in conllx.\n",
    "        '''\n",
    "        buf = []\n",
    "        for line in lines:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            if not line.strip():\n",
    "                if buf:\n",
    "                    yield buf\n",
    "                    buf = []\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                buf.append(line.strip())\n",
    "        if buf:\n",
    "            yield buf\n",
    "\n",
    "    def load_conll_dataset(self, filepath):\n",
    "        '''Reads in a conllx file; generates Observation objects\n",
    "\n",
    "        For each sentence in a conllx file, generates a single Observation\n",
    "        object.\n",
    "\n",
    "        Args:\n",
    "            filepath: the filesystem path to the conll dataset\n",
    "            observation_class: namedtuple for observations\n",
    "\n",
    "        Returns:\n",
    "        A list of Observations\n",
    "        '''\n",
    "        observations = []\n",
    "        lines = (x for x in open(filepath))\n",
    "        for buf in self.generate_lines_for_sent(lines):\n",
    "            conllx_lines = []\n",
    "            for line in buf:\n",
    "                conllx_lines.append(line.strip().split('\\t'))\n",
    "            if self.additional_field_name:\n",
    "                newfield = [None for x in range(len(conllx_lines))]\n",
    "                observation = self.observation_class(\n",
    "                    *zip(*conllx_lines), newfield)\n",
    "            else:\n",
    "                observation = self.observation_class(\n",
    "                    *zip(*conllx_lines))\n",
    "            observations.append(observation)\n",
    "        return observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lemma_sentence</th>\n",
       "      <th>upos_sentence</th>\n",
       "      <th>xpos_sentence</th>\n",
       "      <th>morph</th>\n",
       "      <th>head_indices</th>\n",
       "      <th>governance_relations</th>\n",
       "      <th>secondary_relations</th>\n",
       "      <th>extra_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9)</td>\n",
       "      <td>(We, 're, about, to, see, if, advertising, wor...</td>\n",
       "      <td>(_, _, _, _, _, _, _, _, _)</td>\n",
       "      <td>(PRON, AUX, ADP, PART, VERB, SCONJ, NOUN, VERB...</td>\n",
       "      <td>(PRP, VBP, IN, TO, VB, IN, NN, VBZ, .)</td>\n",
       "      <td>(_, _, _, _, _, _, _, _, _)</td>\n",
       "      <td>(3, 3, 0, 5, 3, 8, 8, 5, 3)</td>\n",
       "      <td>(nsubj, aux, root, aux, xcomp, mark, nsubj, ad...</td>\n",
       "      <td>(_, _, _, _, _, _, _, _, _)</td>\n",
       "      <td>(_, _, _, _, _, _, _, _, _)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>(Odds, and, Ends)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "      <td>(NOUN, CONJ, NOUN)</td>\n",
       "      <td>(NNS, CC, NNS)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "      <td>(0, 1, 1)</td>\n",
       "      <td>(root, cc, conj)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         index  \\\n",
       "0  (1, 2, 3, 4, 5, 6, 7, 8, 9)   \n",
       "1                    (1, 2, 3)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  (We, 're, about, to, see, if, advertising, wor...   \n",
       "1                                  (Odds, and, Ends)   \n",
       "\n",
       "                lemma_sentence  \\\n",
       "0  (_, _, _, _, _, _, _, _, _)   \n",
       "1                    (_, _, _)   \n",
       "\n",
       "                                       upos_sentence  \\\n",
       "0  (PRON, AUX, ADP, PART, VERB, SCONJ, NOUN, VERB...   \n",
       "1                                 (NOUN, CONJ, NOUN)   \n",
       "\n",
       "                            xpos_sentence                        morph  \\\n",
       "0  (PRP, VBP, IN, TO, VB, IN, NN, VBZ, .)  (_, _, _, _, _, _, _, _, _)   \n",
       "1                          (NNS, CC, NNS)                    (_, _, _)   \n",
       "\n",
       "                  head_indices  \\\n",
       "0  (3, 3, 0, 5, 3, 8, 8, 5, 3)   \n",
       "1                    (0, 1, 1)   \n",
       "\n",
       "                                governance_relations  \\\n",
       "0  (nsubj, aux, root, aux, xcomp, mark, nsubj, ad...   \n",
       "1                                   (root, cc, conj)   \n",
       "\n",
       "           secondary_relations                   extra_info  \n",
       "0  (_, _, _, _, _, _, _, _, _)  (_, _, _, _, _, _, _, _, _)  \n",
       "1                    (_, _, _)                    (_, _, _)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CONLL_COLS = ['index',\n",
    "                  'sentence',\n",
    "                  'lemma_sentence',\n",
    "                  'upos_sentence',\n",
    "                  'xpos_sentence',\n",
    "                  'morph',\n",
    "                  'head_indices',\n",
    "                  'governance_relations',\n",
    "                  'secondary_relations',\n",
    "                  'extra_info']\n",
    "\n",
    "OBSERVATIONS = CONLLReader(CONLL_COLS).load_conll_dataset('ptb3-wsj-data/CUSTOM.conllx')\n",
    "OBSERVATIONS2 = CONLLReader(CONLL_COLS).load_conll_dataset('ptb3-wsj-data/CUSTOM4.conllx')\n",
    "\n",
    "pd.DataFrame(OBSERVATIONS).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>lemma_sentence</th>\n",
       "      <th>upos_sentence</th>\n",
       "      <th>xpos_sentence</th>\n",
       "      <th>morph</th>\n",
       "      <th>head_indices</th>\n",
       "      <th>governance_relations</th>\n",
       "      <th>secondary_relations</th>\n",
       "      <th>extra_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>(Odds, and, Ends)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "      <td>(NOUN, CONJ, NOUN)</td>\n",
       "      <td>(NNS, CC, NNS)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "      <td>(0, 1, 1)</td>\n",
       "      <td>(root, cc, conj)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "      <td>(_, _, _)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1, 2, 3, 4, 5, 6, 7, 8)</td>\n",
       "      <td>(Not, his, autograph, ;, power-hitter, McGwire...</td>\n",
       "      <td>(_, _, _, _, _, _, _, _)</td>\n",
       "      <td>(ADV, PRON, NOUN, PUNCT, NOUN, NOUN, PART, PUNCT)</td>\n",
       "      <td>(RB, PRP$, NN, :, NN, NN, POS, .)</td>\n",
       "      <td>(_, _, _, _, _, _, _, _)</td>\n",
       "      <td>(3, 3, 0, 3, 6, 3, 6, 3)</td>\n",
       "      <td>(neg, poss, root, punct, nn, poss, possessive,...</td>\n",
       "      <td>(_, _, _, _, _, _, _, _)</td>\n",
       "      <td>(_, _, _, _, _, _, _, _)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      index  \\\n",
       "0                 (1, 2, 3)   \n",
       "1  (1, 2, 3, 4, 5, 6, 7, 8)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                                  (Odds, and, Ends)   \n",
       "1  (Not, his, autograph, ;, power-hitter, McGwire...   \n",
       "\n",
       "             lemma_sentence  \\\n",
       "0                 (_, _, _)   \n",
       "1  (_, _, _, _, _, _, _, _)   \n",
       "\n",
       "                                       upos_sentence  \\\n",
       "0                                 (NOUN, CONJ, NOUN)   \n",
       "1  (ADV, PRON, NOUN, PUNCT, NOUN, NOUN, PART, PUNCT)   \n",
       "\n",
       "                       xpos_sentence                     morph  \\\n",
       "0                     (NNS, CC, NNS)                 (_, _, _)   \n",
       "1  (RB, PRP$, NN, :, NN, NN, POS, .)  (_, _, _, _, _, _, _, _)   \n",
       "\n",
       "               head_indices  \\\n",
       "0                 (0, 1, 1)   \n",
       "1  (3, 3, 0, 3, 6, 3, 6, 3)   \n",
       "\n",
       "                                governance_relations  \\\n",
       "0                                   (root, cc, conj)   \n",
       "1  (neg, poss, root, punct, nn, poss, possessive,...   \n",
       "\n",
       "        secondary_relations                extra_info  \n",
       "0                 (_, _, _)                 (_, _, _)  \n",
       "1  (_, _, _, _, _, _, _, _)  (_, _, _, _, _, _, _, _)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(OBSERVATIONS2).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8000, 0.1000, 0.1000],\n",
      "        [0.4000, 0.0000, 6.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.Tensor([[.8,.1,.1],[0.4,0,6]])\n",
    "labels = torch.Tensor([0,2]).type(torch.long)\n",
    "print(preds.view(-1,3))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3479)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(preds.view(-1,3),labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0933, -0.5557, -0.4178,  0.7510,  0.9444],\n",
      "        [-0.9207, -0.9642,  1.1006,  0.1601, -0.6287],\n",
      "        [-1.3193, -2.8888,  0.2294,  1.5730,  0.8444]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.7084)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 5)\n",
    "target = torch.empty(3, dtype=torch.long).random_(1)\n",
    "print(input)\n",
    "nn.CrossEntropyLoss()(input,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "SPEC = 'bert-base-uncased'\n",
    "\n",
    "MODEL = AutoModel.from_pretrained(SPEC).to(DEVICE)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(SPEC)\n",
    "POS_VOCABSIZE = 30  # or whatever\n",
    "\n",
    "ARGS = dict(\n",
    "    device=DEVICE,\n",
    "    hidden_dim=MODEL.config.hidden_size,\n",
    "    pos_vocabsize=POS_VOCABSIZE,\n",
    "    epochs=20,\n",
    "    results_path=\"probe-results/\",\n",
    "    corpus=dict(root='ptb3-wsj-data/',\n",
    "                train_path='ptb3-wsj-train.conllx',\n",
    "                dev_path='ptb3-wsj-dev.conllx',\n",
    "                test_path='ptb3-wsj-test.conllx'),\n",
    "    conll_fieldnames=[  # Columns of CONLL file\n",
    "        'index', 'sentence', 'lemma_sentence', 'upos_sentence',\n",
    "        'xpos_sentence', 'morph', 'head_indices',\n",
    "        'governance_relations', 'secondary_relations', 'extra_info']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting observations datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_observations(args):\n",
    "    '''\n",
    "    Get pytorch Datasets for train, dev, test observations\n",
    "    '''\n",
    "    train_corpus_path = os.path.join(\n",
    "        args['corpus']['root'],\n",
    "        args['corpus']['train_path'])\n",
    "    dev_corpus_path = os.path.join(\n",
    "        args['corpus']['root'],\n",
    "        args['corpus']['dev_path'])\n",
    "    test_corpus_path = os.path.join(\n",
    "        args['corpus']['root'],\n",
    "        args['corpus']['test_path'])\n",
    "    reader = CONLLReader(args['conll_fieldnames'])\n",
    "    train_obs = reader.load_conll_dataset(train_corpus_path)\n",
    "    dev_obs = reader.load_conll_dataset(dev_corpus_path)\n",
    "    test_obs = reader.load_conll_dataset(test_corpus_path)\n",
    "\n",
    "    return train_obs, dev_obs, test_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obs, dev_obs, test_obs = load_observations(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>bill</td>\n",
       "      <td>intends</td>\n",
       "      <td>to</td>\n",
       "      <td>restrict</td>\n",
       "      <td>the</td>\n",
       "      <td>RTC</td>\n",
       "      <td>to</td>\n",
       "      <td>Treasury</td>\n",
       "      <td>borrowings</td>\n",
       "      <td>only</td>\n",
       "      <td>,</td>\n",
       "      <td>unless</td>\n",
       "      <td>the</td>\n",
       "      <td>agency</td>\n",
       "      <td>receives</td>\n",
       "      <td>specific</td>\n",
       "      <td>congressional</td>\n",
       "      <td>authorization</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[the]</td>\n",
       "      <td>[bill]</td>\n",
       "      <td>[intends]</td>\n",
       "      <td>[to]</td>\n",
       "      <td>[restrict]</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[rt, ##c]</td>\n",
       "      <td>[to]</td>\n",
       "      <td>[treasury]</td>\n",
       "      <td>[borrowing, ##s]</td>\n",
       "      <td>[only]</td>\n",
       "      <td>[,]</td>\n",
       "      <td>[unless]</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[agency]</td>\n",
       "      <td>[receives]</td>\n",
       "      <td>[specific]</td>\n",
       "      <td>[congressional]</td>\n",
       "      <td>[authorization]</td>\n",
       "      <td>[.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1996]</td>\n",
       "      <td>[3021]</td>\n",
       "      <td>[18754]</td>\n",
       "      <td>[2000]</td>\n",
       "      <td>[21573]</td>\n",
       "      <td>[1996]</td>\n",
       "      <td>[19387, 2278]</td>\n",
       "      <td>[2000]</td>\n",
       "      <td>[9837]</td>\n",
       "      <td>[23733, 2015]</td>\n",
       "      <td>[2069]</td>\n",
       "      <td>[1010]</td>\n",
       "      <td>[4983]</td>\n",
       "      <td>[1996]</td>\n",
       "      <td>[4034]</td>\n",
       "      <td>[8267]</td>\n",
       "      <td>[3563]</td>\n",
       "      <td>[7740]</td>\n",
       "      <td>[20104]</td>\n",
       "      <td>[1012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>TO</td>\n",
       "      <td>VB</td>\n",
       "      <td>DT</td>\n",
       "      <td>NNP</td>\n",
       "      <td>TO</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNS</td>\n",
       "      <td>RB</td>\n",
       "      <td>,</td>\n",
       "      <td>IN</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>PART</td>\n",
       "      <td>VERB</td>\n",
       "      <td>DET</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADV</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1          2       3           4       5              6  \\\n",
       "0     The    bill    intends      to    restrict     the            RTC   \n",
       "1   [the]  [bill]  [intends]    [to]  [restrict]   [the]      [rt, ##c]   \n",
       "2  [1996]  [3021]    [18754]  [2000]     [21573]  [1996]  [19387, 2278]   \n",
       "3      DT      NN        VBZ      TO          VB      DT            NNP   \n",
       "4     DET    NOUN       VERB    PART        VERB     DET          PROPN   \n",
       "\n",
       "        7           8                 9      10      11        12      13  \\\n",
       "0      to    Treasury        borrowings    only       ,    unless     the   \n",
       "1    [to]  [treasury]  [borrowing, ##s]  [only]     [,]  [unless]   [the]   \n",
       "2  [2000]      [9837]     [23733, 2015]  [2069]  [1010]    [4983]  [1996]   \n",
       "3      TO         NNP               NNS      RB       ,        IN      DT   \n",
       "4     ADP       PROPN              NOUN     ADV   PUNCT     SCONJ     DET   \n",
       "\n",
       "         14          15          16               17               18      19  \n",
       "0    agency    receives    specific    congressional    authorization       .  \n",
       "1  [agency]  [receives]  [specific]  [congressional]  [authorization]     [.]  \n",
       "2    [4034]      [8267]      [3563]           [7740]          [20104]  [1012]  \n",
       "3        NN         VBZ          JJ               JJ               NN       .  \n",
       "4      NOUN        VERB         ADJ              ADJ             NOUN   PUNCT  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dev_obs[2]\n",
    "def obs_df(x):\n",
    "    return pd.DataFrame(\n",
    "        (x.sentence,\n",
    "        [TOKENIZER.tokenize(w) for w in x.sentence],\n",
    "        [TOKENIZER.encode(w,add_special_tokens=False) for w in x.sentence],\n",
    "        x.xpos_sentence,\n",
    "        x.upos_sentence,))\n",
    "obs_df(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17 POS tags in the UPOS tagset: ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "sents_UPOS_tags = [o.upos_sentence for o in itertools.chain(train_obs,dev_obs,test_obs)]\n",
    "UPOS_tagset = set(itertools.chain(*sents_UPOS_tags))\n",
    "print(f\"There are {len(UPOS_tagset)} POS tags in the UPOS tagset: {sorted(UPOS_tagset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 POS tags in the UPOS tagset: ['#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "sents_XPOS_tags = [o.xpos_sentence for o in itertools.chain(train_obs,dev_obs,test_obs)]\n",
    "XPOS_tagset = set(itertools.chain(*sents_XPOS_tags))\n",
    "print(f\"There are {len(XPOS_tagset)} POS tags in the UPOS tagset: {sorted(XPOS_tagset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, the loading of a Dataset instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" PyTorch dataloader for POS from Observations.\n",
    "    \"\"\"\n",
    "    def __init__(self, observations, tokenizer, observation_class, POS_set):\n",
    "        '''        Args:\n",
    "            observations: A list of Observations describing a dataset\n",
    "            tokennizer: an instance of a transformers Tokenizer class\n",
    "        '''\n",
    "        self.observations = observations\n",
    "        self.POS_set = POS_set\n",
    "        self.tokenizer = tokenizer\n",
    "        self.POS_to_id = {POS: i for i, POS in enumerate(self.POS_set)}\n",
    "        self.observation_class = observation_class\n",
    "        self.input_ids, self.pos_ids = self.get_input_ids_and_pos_ids()\n",
    "\n",
    "    def sentences_to_idlists(self):\n",
    "        '''Replaces strings in an Observation with lists of integer ids.\n",
    "\n",
    "        Returns:\n",
    "        A list of observations with nested integer-lists for sentence fields\n",
    "        '''\n",
    "        idlist_observations = []\n",
    "        for obs in tqdm(self.observations, desc=\"[getting subtoken ids]\"):\n",
    "            idlist = tuple([self.subword_ids(item) for item in obs.sentence])\n",
    "            idlist_observations.append(self.observation_class(obs[0], idlist, *obs[2:]))\n",
    "        return idlist_observations\n",
    "\n",
    "    def subword_ids(self, item):\n",
    "        '''Gets a list of subword ids for an item (word).'''\n",
    "        return self.tokenizer.encode(item, add_special_tokens=False)\n",
    "\n",
    "    def get_input_ids_and_pos_ids(self):\n",
    "        idlist_observations = self.sentences_to_idlists()\n",
    "        subtoken_id_lists = [obs.sentence for obs in idlist_observations]\n",
    "        pos_label_lists = [obs.xpos_sentence for obs in idlist_observations]\n",
    "        input_ids, pos_ids = self.repeat_POS_to_match(\n",
    "            subtoken_id_lists, pos_label_lists)\n",
    "        return input_ids, pos_ids\n",
    "\n",
    "    def repeat_POS_to_match(self, list_id, list_POS):\n",
    "        assert len(list_POS) == len(list_id), \"list lengths don't match\"\n",
    "        new_id = []\n",
    "        new_POS = []\n",
    "        for i, el_id in enumerate(list_id):\n",
    "            newlist_id = []\n",
    "            newlist_POS = []\n",
    "            for j, elel_id in enumerate(el_id):\n",
    "                for token_id in elel_id:\n",
    "                    newlist_id.append(token_id)\n",
    "                    newlist_POS.append(self.POS_to_id[list_POS[i][j]])\n",
    "            new_id.append(newlist_id)\n",
    "            new_POS.append(newlist_POS)\n",
    "        return new_id, new_POS\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.pos_ids[index]\n",
    "\n",
    "def load_datasets(args, tokenizer):\n",
    "    '''\n",
    "    Get pytorch Datasets for train, dev, test observations\n",
    "    '''\n",
    "    train_corpus_path = os.path.join(\n",
    "        args['corpus']['root'],\n",
    "        args['corpus']['train_path'])\n",
    "    dev_corpus_path = os.path.join(\n",
    "        args['corpus']['root'],\n",
    "        args['corpus']['dev_path'])\n",
    "    test_corpus_path = os.path.join(\n",
    "        args['corpus']['root'],\n",
    "        args['corpus']['test_path'])\n",
    "    reader = CONLLReader(args['conll_fieldnames'])\n",
    "#     train_obs = reader.load_conll_dataset(train_corpus_path)\n",
    "    dev_obs = reader.load_conll_dataset(dev_corpus_path)\n",
    "#     test_obs = reader.load_conll_dataset(test_corpus_path)\n",
    "\n",
    "    obs_class = reader.observation_class\n",
    "#     train_dataset = POSDataset(train_obs, tokenizer, obs_class)\n",
    "    dev_dataset = POSDataset(dev_obs, tokenizer, obs_class, POS_set)\n",
    "#     test_dataset = POSDataset(test_obs, tokenizer, obs_class)\n",
    "\n",
    "    return dev_dataset #train_dataset, dev_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[getting subtoken ids]: 100%|██████████| 1700/1700 [00:02<00:00, 720.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_dataset, dev_dataset, test_dataset = load_datasets(ARGS, TOKENIZER)\n",
    "dev_dataset = load_datasets(ARGS, TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0       1             2       3       4         5        6  \\\n",
      "0            ``       I            'm     for     the    Giants    today   \n",
      "1        [`, `]     [i]        [', m]   [for]   [the]  [giants]  [today]   \n",
      "2  [1036, 1036]  [1045]  [1005, 1049]  [2005]  [1996]    [7230]   [2651]   \n",
      "3            ``     PRP           VBP      IN      DT       NNP       NN   \n",
      "4         PUNCT    PRON          VERB     ADP     DET     PROPN     NOUN   \n",
      "\n",
      "        7       8       9         10      11      12           13      14  \n",
      "0       ,     but    only    because    they    lost    yesterday       .  \n",
      "1     [,]   [but]  [only]  [because]  [they]  [lost]  [yesterday]     [.]  \n",
      "2  [1010]  [2021]  [2069]     [2138]  [2027]  [2439]       [7483]  [1012]  \n",
      "3       ,      CC      RB         IN     PRP     VBD           NN       .  \n",
      "4   PUNCT    CONJ     ADV      SCONJ    PRON    VERB         NOUN   PUNCT  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>`</td>\n",
       "      <td>``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>`</td>\n",
       "      <td>``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>giants</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>today</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>but</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>only</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>because</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lost</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yesterday</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1\n",
       "0           `   ``\n",
       "1           `   ``\n",
       "2           i  PRP\n",
       "3           '  VBP\n",
       "4           m  VBP\n",
       "5         for   IN\n",
       "6         the   DT\n",
       "7      giants  NNP\n",
       "8       today   NN\n",
       "9           ,    ,\n",
       "10        but   CC\n",
       "11       only   RB\n",
       "12    because   IN\n",
       "13       they  PRP\n",
       "14       lost  VBD\n",
       "15  yesterday   NN\n",
       "16          .    ."
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking:\n",
    "index = 101\n",
    "print(obs_df(dev_obs[index]))\n",
    "pd.DataFrame(zip(\n",
    "    [TOKENIZER.convert_ids_to_tokens([i])[0] for i in dev_dataset[index][0]],\n",
    "    [id_to_POS[i] for i in dev_dataset[index][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_set = ['#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':',\n",
    "                   'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR',\n",
    "                   'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT',\n",
    "                   'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM',\n",
    "                   'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "                   'WDT', 'WP', 'WP$', 'WRB', '``']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_set.index('``')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_to_id = {k: v for v, k in enumerate(POS_set)}\n",
    "id_to_POS = {v: k for k, v in POS_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtoken_idlists  = [([1], [100,101], [16]),\n",
    "            ([50], [17])]\n",
    "\n",
    "POSlists = [(\"DT\", \"NN\", \"VBZ\"),\n",
    "            (\"NNP\", \"VB\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 100, 101, 16], [50, 17]]\n",
      "[[10, 19, 19, 39], [20, 34]]\n"
     ]
    }
   ],
   "source": [
    "def repeat_POS_to_match(list_id, list_POS):\n",
    "    assert len(list_POS) == len(list_id), \"list lengths don't match\"\n",
    "    new_id = []\n",
    "    new_POS = []\n",
    "    for i, el_id in enumerate(list_id):\n",
    "        newlist_id = []\n",
    "        newlist_POS = []\n",
    "        for j, elel_id in enumerate(el_id):\n",
    "            for token_id in elel_id:\n",
    "                newlist_id.append(token_id)\n",
    "                newlist_POS.append(POS_to_id[list_POS[i][j]])\n",
    "        new_id.append(newlist_id)\n",
    "        new_POS.append(newlist_POS)\n",
    "    print(new_id)\n",
    "    print(new_POS)\n",
    "\n",
    "repeat_POS_to_match(subtoken_idlists,POSlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[10], [19, 19], [39]], [[20], [34]]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repeat_POS_to_match2(POSlists, subtoken_id_lists):\n",
    "    l = [[[POS_to_id[POSlists[i][j]] for _ in idlist] \n",
    "          for j, idlist in enumerate(sent)]\n",
    "         for i, sent in enumerate(subtoken_id_lists)]\n",
    "    return l\n",
    "\n",
    "repeat_POS_to_match2(POSlists,subtoken_idlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'odds', 'not', 'frankfurt', 'other', 'bits', 'we', '`', 'at', 'yes']\n",
      "[\"'\", 'and', 'his', ':', 'broker', 'and', \"'\", '`', 'applied', 'and']\n",
      "['PRP', 'NNS', 'RB', 'NNP', 'JJ', 'NNS', 'PRP', '``', 'IN', 'NNS']\n",
      "['VBP', 'CC', 'PRP$', ':', 'NN', 'CC', 'VBP', '``', 'NNP', 'CC']\n"
     ]
    }
   ],
   "source": [
    "print(TOKENIZER.convert_ids_to_tokens([2057, 10238,  2025,  9780,  2060,  9017,  2057,  1036,  2012,  2748]))\n",
    "print(TOKENIZER.convert_ids_to_tokens([1005,  1998,  2010,  1024, 20138,  1998,  1005,  1036,  4162,  1998]))\n",
    "\n",
    "print([id_to_POS[x] for x in [25, 22, 27, 20, 14, 22, 25, 44, 13, 22]])\n",
    "print([id_to_POS[x] for x in [38,  8, 26,  7, 19,  8, 38, 44, 20,  8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[([ 2057, 10238,  2025,  9780,  2060,  9017,  2057,  1036,  2012,  2748]), ([ 1005,  1998,  2010,  1024, 20138,  1998,  1005,  1036,  4162,  1998])], [([25, 22, 27, 20, 14, 22, 25, 44, 13, 22]), ([38,  8, 26,  7, 19,  8, 38, 44, 20,  8])], ([10,  3, 14,  2, 20,  3, 10, 22, 13,  3])]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102],\n",
      "        [  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]) torch.Size([2, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 8, 768]), torch.Size([2, 768])]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = TOKENIZER.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs = torch.stack((inputs,inputs), dim=1).view(2,-1)\n",
    "print('inputs:',inputs, inputs.shape)\n",
    "\n",
    "outputs = MODEL(inputs)\n",
    "last_hidden_states = outputs[0] \n",
    "[x.size() for x in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.empty((5,10))==0).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.encode(\"a compositional phrase.\")\n",
    "POS_to_id[\".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should do it like:\n",
    "```\n",
    "input tokens = ['[CLS]',  'a', 'composition', '##al', 'phrase',  '.', '[SEP]', PADDING, ..., PADDING ]\n",
    "POS labels   = [    pad, 'DT',          'JJ',    'JJ,     'NN',  '.',     pad,     pad, ...,     pad ]\n",
    "```\n",
    "which is input like\n",
    "```\n",
    "input_ids    = [    101, 1037,          5512,   2389,     7655, 1012,     102,      -1, ...       -1 ]\n",
    "POS_ids      = [     -1,   10,            14,     14,       19,    6,      -1,      -1, ...,      -1 ]\n",
    "```            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin',\n",
       " 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin',\n",
       " 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin',\n",
       " 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin',\n",
       " 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin',\n",
       " 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin',\n",
       " 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin',\n",
       " 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin',\n",
       " 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin',\n",
       " 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin',\n",
       " 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin',\n",
       " 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-pytorch_model.bin',\n",
       " 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-pytorch_model.bin',\n",
       " 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-pytorch_model.bin',\n",
       " 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-pytorch_model.bin',\n",
       " 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-pytorch_model.bin',\n",
       " 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-pytorch_model.bin',\n",
       " 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/pytorch_model.bin',\n",
       " 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/pytorch_model.bin',\n",
       " 'bert-base-dutch-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/pytorch_model.bin'}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.pretrained_model_archive_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20.07.10-08.13'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "now.strftime(\"%y.%m.%d-%H.%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-07-10 08:16:23'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "NOW = time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200710-081716'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOW.strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[4., 2., 3., 3., 2., 4.],\n",
       "        [2., 2., 5., 2., 2., 2.]]),\n",
       "indices=tensor([[5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 3, 5, 3]]))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor(\n",
    "    [[[-5,1,0,2,1,4,1],\n",
    "      [-6,1,0,2,1,2,1],\n",
    "      [-5,1,0,2,1,3,1],\n",
    "      [-5,1,0,2,1,3,1],\n",
    "      [-6,1,0,2,1,2,1],\n",
    "      [-5,1,0,2,1,4,1]],\n",
    "     \n",
    "     [[-5,1,0,2,1,2,1],\n",
    "      [-6,1,0,2,1,2,1],\n",
    "      [-5,1,0,2,1,5,1],\n",
    "      [-5,1,0,2,1,1,1],\n",
    "      [-6,1,0,2,1,2,1],\n",
    "      [-5,1,0,2,1,1,1]]])\n",
    "print(t.size())\n",
    "torch.max(t,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 3, 5, 3]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t.view(-1).eq(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0909090909090908"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(sum(t.view(-1)==t.view(-1)/2)) / 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(72.)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t != 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(72.)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t.ne(0)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(-1,12).argmax(-1).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1429)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t == t/2).sum().float() / t.eq(t).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argument: 'value'\n",
      "another_arg: 2\n",
      "subdict: \n",
      "    arg1: 'value'\n",
      "    arg2: 'moope'\n",
      "    e2: 0.6666666666666666\n",
      "listarg: ['this', 'is', 'listed']\n",
      "device: device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "ARGS = dict(\n",
    "    argument='value',\n",
    "    another_arg=2,\n",
    "    subdict=dict(\n",
    "        arg1='value',\n",
    "        arg2='moope',\n",
    "        e2=2/3\n",
    "    ),\n",
    "    listarg=['this', 'is', 'listed'],\n",
    "    device=t.device\n",
    ")\n",
    "\n",
    "def pretty_dict(d, indent=0): \n",
    "    for key, value in d.items():\n",
    "        print('    '*indent + key, end=': ')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            pretty_dict(value, indent+1)\n",
    "        else:\n",
    "            print(repr(value))\n",
    "        \n",
    "pretty_dict(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f10c0bb50f4f93be4494423fec357c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello 0\n",
      "Hello 1\n",
      "Hello 2\n",
      "Hello 3\n",
      "Hello 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from tqdm.utils import _term_move_up\n",
    "\n",
    "\n",
    "for ii in tqdm(range(5),position=2):\n",
    "    tqdm.write(f'Hello {ii}',nolock=True)\n",
    "    sleep(0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "size = 3\n",
    "if np.random.randint(0, 2):\n",
    "    print(\"yes\")\n",
    "    print(np.random.randint(0, 2,size=size))\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pmienv] *",
   "language": "python",
   "name": "conda-env-pmienv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
